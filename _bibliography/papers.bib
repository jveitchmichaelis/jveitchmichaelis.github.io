---
---

@article{veitchmichaelis2024oamtcdgloballydiversedataset,
  abbr = {Preprint},
      title={OAM-TCD: A globally diverse dataset of high-resolution tree cover maps}, 
      author={Josh Veitch-Michaelis and Andrew Cottam and Daniella Schweizer and Eben N. Broadbent and David Dao and Ce Zhang and Angelica Almeyda Zambrano and Simeon Max},
      year={2024},
      eprint={2407.11743},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
	  Publisher = {arXiv preprint, currently under peer review},
    Abstract={Accurately quantifying tree cover is an important metric for ecosystem monitoring and for assessing progress in restored sites. Recent works have shown that deep learning-based segmentation algorithms are capable of accurately mapping trees at country and continental scales using high-resolution aerial and satellite imagery. Mapping at high (ideally sub-meter) resolution is necessary to identify individual trees, however there are few open-access datasets containing instance level annotations and those that exist are small or not geographically diverse. We present a novel open-access dataset for individual tree crown delineation (TCD) in high-resolution aerial imagery sourced from OpenAerialMap (OAM). Our dataset, OAM-TCD, comprises 5072 2048x2048 px images at 10 cm/px resolution with associated human-labeled instance masks for over 280k individual and 56k groups of trees. By sampling imagery from around the world, we are able to better capture the diversity and morphology of trees in different terrestrial biomes and in both urban and natural environments. Using our dataset, we train reference instance and semantic segmentation models that compare favorably to existing state-of-the-art models. We assess performance through k-fold cross-validation and comparison with existing datasets; additionally we demonstrate compelling results on independent aerial imagery captured over Switzerland and compare to municipal tree inventories and LIDAR-derived canopy maps in the city of Zurich. Our dataset, models and training/benchmark code are publicly released under permissive open-source licenses: Creative Commons (majority CC BY 4.0), and Apache 2.0 respectively. },
	  DOI = {10.48550/arXiv.2407.11743},
}

@article{PPR:PPR534649,
  abbr = {Sci. Rep.},
  Title = {In-orbit demonstration of a re-trainable Machine Learning Payload for processing optical imagery},
  Author = {Mateo-García, Gonzalo and Josh Veitch-Michaelis and Purcell, Cormac and Longepe, Nicolas and Mathieu, Pierre Philippe and Reid, Simon and Anlind, Alice and Bruhn, Fredrik and Parr, James},
  DOI = {10.10238/s41598-023-34436-w},
  Abstract = {Cognitive cloud computing in space (3CS) describes a new frontier of space innovation powered by Artificial Intelligence, enabling an explosion of new applications in observing our planet and enabling deep space exploration. In this framework, machine learning (ML) payloads --isolated software capable of extracting high level information from onboard sensors-- are key to accomplish this vision. In this work we demonstrate, in a satellite deployed in orbit, a ML payload called ‘WorldFloods’ that is able to send compressed flood maps from sensed images. In particular, we perform a set of experiments to: (1) compare different segmentation models on different processing variables critical for onboard deployment, (2) show that we can produce, onboard, vectorised polygons delineating the detected flood water from a full Sentinel-2 tile, (3) retrain the model with few images of the onboard sensor downlinked to Earth and (4) demonstrate that this new model can be uplinked to the satellite and run on new images acquired by its camera. Overall our work demonstrates that ML-based models deployed in orbit can be updated if new information is available, paving the way for agile integration of onboard and onground processing and "on the fly" continuous learning.},
  journal = {Scientific Reports},
  volume={13},
  number={10391},
  Year = {2023}
}

@article{2021worldfloods,
  abbr={Sci. Rep.},
  title={Towards global flood mapping onboard low cost satellites with machine learning },
  author={Mateo-Garcia, Gonzalo and Veitch-Michaelis, Josh and Smith, Lewis and Oprea, Silviu  and Schumann, Guy and Gal, Yarin and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Backes, Dietmar},
  journal={Scientific Reports},
  volume={11},
  number={7429},
  year={2021},
  doi={10.1038/s41598-021-86650-z},
  abstract = {Spaceborne Earth observation is a key technology for flood response, offering valuable information to decision makers on the ground. Very large constellations of small, nano satellites— 'CubeSats' are a promising solution to reduce revisit time in disaster areas from days to hours. However, data transmission to ground receivers is limited by constraints on power and bandwidth of CubeSats. Onboard processing offers a solution to decrease the amount of data to transmit by reducing large sensor images to smaller data products. The ESA's recent PhiSat-1 mission aims to facilitate the demonstration of this concept, providing the hardware capability to perform onboard processing by including a power-constrained machine learning accelerator and the software to run custom applications. This work demonstrates a flood segmentation algorithm that produces flood masks to be transmitted instead of the raw images, while running efficiently on the accelerator aboard the PhiSat-1. Our models are trained on WorldFloods: a newly compiled dataset of 119 globally verified flooding events from disaster response organizations, which we make available in a common format. We test the system on independent locations, demonstrating that it produces fast and accurate segmentation masks on the hardware accelerator, acting as a proof of concept for this approach.}
}

@article{nature_iayc,
  abbr={Nat. Astron.},
  author={Dalgleish, Hannah and Veitch-Michaelis, Josh}, 
  year={2019},
  volume={3},
  pages={1043--1047},
  journal={Nature Astronomy},
  title={Assessing the influence of one astronomy camp over 50 years},
  doi={10.1038/s41550-019-0965-y},
  abstract={The International Astronomical Youth Camp has benefited thousands of lives during its 50 year history. We explore the pedagogy behind this success, review a survey taken by more than 300 previous participants and discuss some of the challenges the camp faces in the future.}
}

@inproceedings{mateo2019flood,
  abbr={NeurIPS},
  title={Flood Detection On Low Cost Orbital Hardware},
  author={Mateo-Garcia, Gonzalo and Oprea, Silviu and Smith, Lewis and Veitch-Michaelis, Josh and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Backes, Dietmar and Gal, Yarin and Schumann, Guy},
  booktitle={Artificial Intelligence for Humanitarian Assistance and Disaster Response Workshop at NeurIPS},
  year={2019},
  doi={10.48550/arXiv.1910.03019},
  abstract={Satellite imaging is a critical technology for monitoring and responding to natural disasters such as flooding. Despite the capabilities of modern satellites, there is still much to be desired from the perspective of first response organisations like UNICEF. Two main challenges are rapid access to data, and the ability to automatically identify flooded regions in images. We describe a prototypical flood segmentation system, identifying cloud, water and land, that could be deployed on a constellation of small satellites, performing processing on board to reduce downlink bandwidth by 2 orders of magnitude. We target PhiSat-1, part of the FSSCAT mission, which is planned to be launched by the European Space Agency (ESA) near the start of 2020 as a proof of concept for this new technology. },
  website={https://www.hadr.ai/previous-years/2019/home-2019}
}

@inproceedings{mcwhirter2019saving,
  abbr={ADASS 2019},
  title={Saving Endangered Animals with Astro-Ecology},
  author={McWhirter, Paul Ross and Veitch-Michaelis, Josh},
  booktitle={Astronomical Data Analysis Software and Systems XXVII},
  volume={523},
  pages={95},
  year={2019},
  abstract={Conservation science is experiencing an unprecedented challenge in identifying and protecting endangered species across the world. The large stretches of land and sea require innovative solutions for the monitoring of endangered populations. Drones equipped with high resolution cameras with supporting data from satellites have helped to mitigate these challenges. Unfortunately, it is difficult to detect animals from optical images when they might only be a matter of a few pixels across. By deploying thermal infrared cameras on drones to detect animals from their body heat, they can be detected despite their small size in the images. In the thermal infrared band, animals appear as bright sources on a dark, colder background. Through the use of astronomical source detection techniques, these bright animals can be detected although other warm objects lead to false detections. In this paper we demonstrate a technique which uses modern computer vision to build on astronomical source detection algorithms to create a model for the detection and classification of animal thermal profiles in the presence of other warm objects. Using a dataset from Chester Zoo in the UK, we trained a model using 972 frames from a video of the chimpanzee enclosure and achieved excellent results with a training loss of 0.81 and minimal false detections of warm enviromental sources.},
  website={https://ui.adsabs.harvard.edu/abs/2019ASPC..523...95M}
}

@inproceedings{rascaladass,
  abbr={ADASS},
  title={RASCAL: Towards automated spectral wavelength calibration},
  author={Veitch-Michaelis, Josh and Lam, Marco},
  booktitle={"Astronomical Data Analysis Software and Systems XXVIII"},
  year={2020},
  abstract={Wavelength calibration is a routine and critical part of any spectral work-flow, but many astronomers still resort to matching detected peaks and emission lines by hand. We present RASCAL (RANSAC Assisted Spectral CALibration), a python library for automated wavelength calibration of astronomical spectrographs. RASCAL implements recent state-of-the-art methods for wavelength calibration and requires minimal input from a user. In this paper we discuss the implementation of the library and apply it to real-world calibration spectra.},
  doi={10.48550/arXiv.1912.05883},
  website={https://github.com/jveitchmichcaelis/rascal}
}

@inproceedings{dalgleish2018international,
  abbr={CAP},
  title={The International Astronomical Youth Camp: Lessons Learned in 50 Years},
  author={Dalgleish, Hannah and Veitch-Michaelis, Josh},
  booktitle={Communicating Astronomy with the Public Conference 2018 2nd Edition},
  pages={206--207},
  year={2018}
}

@inproceedings{veitch2016enhancement,
  abbr={Int. Arch. ISPRS},
  title={Enhancement of stereo imagery by artificial texture projection generated using a LIDAR},
  author={Veitch-Michaelis, Josh and Muller, Jan-Peter and Walton, David and Storey, Jonathan and Foster, Michael and Crutchley, Benjamin},
  booktitle={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  volume={41},
  pages={599--606},
  year={2016},
  organization={Copernicus Gesellschaft MBH}
}

@article{tao2018massive,
  abbr={Planet. Space Sci.},
  title={Massive stereo-based DTM production for Mars on cloud computers},
  author={Tao, Yu and Muller, Jan-Peter and Sidiropoulos, Panos and Xiong, Si-Ting and Putri, ARD and Walter, SHG and Veitch-Michaelis, Josh and Yershov, Vladimir},
  journal={Planetary and Space Science},
  volume={154},
  pages={30--58},
  year={2018},
  publisher={}
}

@inproceedings{veitch2016crack,
  abbr={CRV},
  title={Crack Detection in ``As-Cast'' Steel Using Laser Triangulation and Machine Learning},
  author={Veitch-Michaelis, Josh and Tao, Yu and Walton, Dave and Muller, Jan-Peter and Crutchley, Benjamin and Storey, Jonathan and Paterson, Christopher and Chown, Andrew},
  booktitle={13th Conference on Computer and Robot Vision (CRV)},
  pages={342--349},
  year={2016},
  organization={IEEE}
}

@phdthesis{veitch2017fusion,
  abbr={PhD Thesis},
  title={Fusion of LIDAR with stereo camera data - an assessment},
  author={Veitch-Michaelis, Josh},
  year={2017},
  school={UCL (University College London)},
  url={https://discovery.ucl.ac.uk/1536083/},
  abstract={This thesis explores data fusion of LIDAR (laser range-finding) with stereo matching, with a particular emphasis on close-range industrial 3D imaging. Recently there has been interest in improving the robustness of stereo matching using data fusion with active range data. These range data have typically been acquired using time of flight cameras (ToFCs), however ToFCs offer poor spatial resolution and are noisy. Comparatively little work has been performed using LIDAR. It is argued that stereo and LIDAR are complementary and there are numerous advantages to integrating LIDAR into stereo systems. For instance, camera calibration is a necessary prerequisite for stereo 3D reconstruction, but the process is often tedious and requires precise calibration targets. It is shown that a visible-beam LIDAR enables automatic, accurate (sub-pixel) extrinsic and intrinsic camera calibration without any explicit targets. Two methods for using LIDAR to assist dense disparity maps from featureless scenes were investigated. The first involved using a LIDAR to provide high-confidence seed points for a region growing stereo matching algorithm. It is shown that these seed points allow dense matching in scenes which fail to match using stereo alone. Secondly, LIDAR was used to provide artificial texture in featureless image regions. Texture was generated by combining real or simulated images of every point the laser hits to form a pseudo-random pattern. Machine learning was used to determine the image regions that are most likely to be stereo- matched, reducing the number of LIDAR points required. Results are compared to competing techniques such as laser speckle, data projection and diffractive optical elements.}
}

@inproceedings{tao2016optimised,
  abbr={Int. Arch. ISPSRS},
  title={An Optimised System for Generating Multi-Resolution DTMs using NASA MRO Datasets},
  author={Tao, Yu and Muller, Jan-Peter and Sidiropoulos, Panos and Veitch-Michaelis, Josh and Yershov, Vladimir},
  booktitle={International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences},
  year={2016}
}

